{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6000b989-5a62-4003-8a19-4745c99a53e8",
   "metadata": {},
   "source": [
    "# Topics In AI Final Report Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9a12c-92ef-4f48-9838-b9b7b15463c9",
   "metadata": {},
   "source": [
    "## Lexical Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f3c09-4da4-491d-af83-6edd6fc43e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('hippoCorpusV2.csv')\n",
    "\n",
    "# Feature extraction functions\n",
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))\n",
    "\n",
    "def average_word_length(story):\n",
    "    words = story.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def lexical_diversity(story):\n",
    "    words = story.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def punctuation_count(story):\n",
    "    return sum(1 for char in story if char in string.punctuation)\n",
    "\n",
    "def average_sentence_length(story):\n",
    "    words = story.split()\n",
    "    sentences = sent_tokenize(story)\n",
    "    return len(words) / len(sentences) if sentences else 0\n",
    "\n",
    "def count_sensory_words(story, sensory_words):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in sensory_words for word in words)\n",
    "\n",
    "def count_first_person_pronouns(story, pronouns):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in pronouns for word in words)\n",
    "\n",
    "def count_emotion_sentences(stories):\n",
    "    return sum(TextBlob(sentence).sentiment.polarity != 0 for sentence in sent_tokenize(stories))\n",
    "\n",
    "def count_dialogue_tags(story, tags):\n",
    "    words = word_tokenize(story)\n",
    "    tagged_words = pos_tag(words)\n",
    "    return sum(tag.startswith('VB') and word.lower() in tags for word, tag in tagged_words)\n",
    "\n",
    "def count_past_tense_verbs(story):\n",
    "    words = word_tokenize(story)\n",
    "    tagged_words = pos_tag(words)\n",
    "    return sum(tag in ['VBD', 'VBN'] for word, tag in tagged_words)\n",
    "\n",
    "# Function to calculate all features for a list of stories\n",
    "def calculate_all_features(stories):\n",
    "    features = {}\n",
    "    sensory_words = ['see', 'hear', 'touch', 'taste', 'smell']\n",
    "    pronouns = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    dialogue_tags = ['say', 'ask', 'reply', 'yell', 'whisper']\n",
    "    \n",
    "    for feature_func in [count_words, count_sentences, average_word_length, \n",
    "                         lexical_diversity, punctuation_count, average_sentence_length]:\n",
    "        features[feature_func.__name__] = [feature_func(story) for story in stories]\n",
    "    \n",
    "    features['sensory_word_count'] = [count_sensory_words(story, sensory_words) for story in stories]\n",
    "    features['first_person_pronoun_count'] = [count_first_person_pronouns(story, pronouns) for story in stories]\n",
    "    features['emotion_sentence_count'] = [count_emotion_sentences(story) for story in stories]\n",
    "    features['dialogue_tag_count'] = [count_dialogue_tags(story, dialogue_tags) for story in stories]\n",
    "    features['past_tense_verb_count'] = [count_past_tense_verbs(story) for story in stories]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Main function to perform analyses and plot results\n",
    "def main(df):\n",
    "    imagined_stories = df[df['memType'] == 'imagined']['story']\n",
    "    recalled_stories = df[df['memType'] == 'recalled']['story']\n",
    "\n",
    "    all_imagined_analysis = calculate_all_features(imagined_stories)\n",
    "    all_recalled_analysis = calculate_all_features(recalled_stories)\n",
    "\n",
    "# Call the main function\n",
    "\n",
    "df1=pd.read_csv(\"hippoCorpusV2.csv\");\n",
    "main(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb38b8-ecf5-40d0-9dd4-1e1e520f743f",
   "metadata": {},
   "source": [
    "## Classifier model with 2 Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b53e4-fd6a-4afa-95f0-547427d9b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19d020-ac7e-4773-b8c9-56fa9d716684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "def init_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, model, tokenizer, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "# Function to add probabilities to DataFrame\n",
    "def add_probabilities_to_df(df, index, probabilities):\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "    return df\n",
    "\n",
    "# Function to calculate Sequentiality scores\n",
    "def calculate_sequentiality(df, history_sizes):\n",
    "    for h in history_sizes:\n",
    "        if h == 0: continue\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df[seq_key] = df[f'probability_history_size{h}'] - df['probability_history_size0']\n",
    "    return df\n",
    "\n",
    "# Function to extract linguistic features\n",
    "def extract_linguistic_features(df):\n",
    "    df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    return df\n",
    "\n",
    "# Function to train the classification model and validate it\n",
    "def train_and_validate(X_train, X_val, y_train, y_val):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train.fillna(0), y_train)\n",
    "    y_pred = clf.predict(X_val.fillna(0))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def run_pipeline():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"hippoCorpusV2.csv\").head(3500)\n",
    "\n",
    "    # Initialize model\n",
    "    model, tokenizer = init_model()\n",
    "\n",
    "    # Process each story and calculate probabilities and Sequentiality scores\n",
    "    history_sizes = [0, 1, 2, 3, 4, 5]\n",
    "    for index, row in df.iterrows():\n",
    "        probabilities = calculate_probabilities(row['story'], model, tokenizer, history_sizes)\n",
    "        df = add_probabilities_to_df(df, index, probabilities)\n",
    "    df = calculate_sequentiality(df, history_sizes)\n",
    "\n",
    "    # Extract linguistic features\n",
    "    df = extract_linguistic_features(df)\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    feature_cols = ['word_count', 'sentence_count'] + \\\n",
    "                   [f'probability_history_size{i}' for i in history_sizes] + \\\n",
    "                   [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and validate the classification model\n",
    "    accuracy = train_and_validate(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c0b1b-e4e7-4e23-aebe-2411862c44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145203-421b-4702-8c00-6d6f3172a3bc",
   "metadata": {},
   "source": [
    "## Classifier model with 4 Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205ef05-e877-4dbd-a286-4ba7a43b9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))\n",
    "\n",
    "def average_word_length(story):\n",
    "    words = story.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def punctuation_count(story):\n",
    "    return sum(1 for char in story if char in string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e9665-200a-4a61-aa13-6cd371071eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "def init_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, model, tokenizer, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "# Function to add probabilities to DataFrame\n",
    "def add_probabilities_to_df(df, index, probabilities):\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "    return df\n",
    "\n",
    "# Function to calculate Sequentiality scores\n",
    "def calculate_sequentiality(df, history_sizes):\n",
    "    for h in history_sizes:\n",
    "        if h == 0: continue\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df[seq_key] = df[f'probability_history_size{h}'] - df['probability_history_size0']\n",
    "    return df\n",
    "\n",
    "# Function to extract linguistic features\n",
    "def extract_linguistic_features(df):\n",
    "    df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['avg_word_length'] = df['story'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "    df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "    return df\n",
    "\n",
    "# Function to train the classification model and validate it\n",
    "def train_and_validate(X_train, X_val, y_train, y_val):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train.fillna(0), y_train)\n",
    "    y_pred = clf.predict(X_val.fillna(0))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def run_pipeline():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"hippoCorpusV2.csv\").head(3500)\n",
    "\n",
    "    # Initialize model\n",
    "    model, tokenizer = init_model()\n",
    "\n",
    "    # Process each story and calculate probabilities and Sequentiality scores\n",
    "    history_sizes = [0, 1, 2, 3, 4, 5]\n",
    "    for index, row in df.iterrows():\n",
    "        probabilities = calculate_probabilities(row['story'], model, tokenizer, history_sizes)\n",
    "        df = add_probabilities_to_df(df, index, probabilities)\n",
    "    df = calculate_sequentiality(df, history_sizes)\n",
    "\n",
    "    # Extract linguistic features\n",
    "    df = extract_linguistic_features(df)\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    feature_cols = ['word_count', 'sentence_count', 'avg_word_length', 'punctuation_count'] + \\\n",
    "                   [f'probability_history_size{i}' for i in history_sizes] + \\\n",
    "                   [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and validate the classification model\n",
    "    accuracy = train_and_validate(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b04b4c-1d51-4b46-8182-44b0c6a5ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277a853-9188-47b4-9e58-e1bb96047693",
   "metadata": {},
   "source": [
    "## Classifier model with 6 Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8ab6e-b2e4-43e5-b315-5bcd6a3a9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))\n",
    "\n",
    "def average_word_length(story):\n",
    "    words = story.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def lexical_diversity(story):\n",
    "    words = story.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def punctuation_count(story):\n",
    "    return sum(1 for char in story if char in string.punctuation)\n",
    "\n",
    "def average_sentence_length(story):\n",
    "    words = story.split()\n",
    "    sentences = sent_tokenize(story)\n",
    "    return len(words) / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176c26f-e5c2-43c2-933d-11d265be7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "def init_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, model, tokenizer, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "# Function to add probabilities to DataFrame\n",
    "def add_probabilities_to_df(df, index, probabilities):\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "    return df\n",
    "\n",
    "# Function to calculate Sequentiality scores\n",
    "def calculate_sequentiality(df, history_sizes):\n",
    "    for h in history_sizes:\n",
    "        if h == 0: continue\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df[seq_key] = df[f'probability_history_size{h}'] - df['probability_history_size0']\n",
    "    return df\n",
    "\n",
    "# Function to extract linguistic features\n",
    "def extract_linguistic_features(df):\n",
    "    df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "    df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "    df['avg_sentence_length'] = df['story'].apply(lambda x: np.mean([len(sentence.split()) for sentence in nltk.sent_tokenize(x)]) if nltk.sent_tokenize(x) else 0)\n",
    "    df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['avg_word_length'] = df['story'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "    return df\n",
    "\n",
    "# Function to train the classification model and validate it\n",
    "def train_and_validate(X_train, X_val, y_train, y_val):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train.fillna(0), y_train)\n",
    "    y_pred = clf.predict(X_val.fillna(0))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def run_pipeline():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"hippoCorpusV2.csv\").head(3500)\n",
    "\n",
    "    # Initialize model\n",
    "    model, tokenizer = init_model()\n",
    "\n",
    "    # Process each story and calculate probabilities and Sequentiality scores\n",
    "    history_sizes = [0, 1, 2, 3, 4, 5]\n",
    "    for index, row in df.iterrows():\n",
    "        probabilities = calculate_probabilities(row['story'], model, tokenizer, history_sizes)\n",
    "        df = add_probabilities_to_df(df, index, probabilities)\n",
    "    df = calculate_sequentiality(df, history_sizes)\n",
    "\n",
    "    # Extract linguistic features\n",
    "    df = extract_linguistic_features(df)\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    feature_cols = ['word_count', 'sentence_count', 'avg_word_length', 'lexical_diversity', 'punctuation_count','avg_sentence_length'] + \\\n",
    "                   [f'probability_history_size{i}' for i in history_sizes] + \\\n",
    "                   [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and validate the classification model\n",
    "    accuracy = train_and_validate(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d23b5-44f3-44be-bbed-8aaa5a924a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdea458-d831-4ad8-a8bd-156a46f38629",
   "metadata": {},
   "source": [
    "## Classifier model with 8 Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb14ce-8109-474c-8ac0-661a02e4b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))\n",
    "\n",
    "def average_word_length(story):\n",
    "    words = story.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def lexical_diversity(story):\n",
    "    words = story.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def punctuation_count(story):\n",
    "    return sum(1 for char in story if char in string.punctuation)\n",
    "\n",
    "def average_sentence_length(story):\n",
    "    words = story.split()\n",
    "    sentences = sent_tokenize(story)\n",
    "    return len(words) / len(sentences) if sentences else 0\n",
    "\n",
    "def count_sensory_words(story, sensory_words):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in sensory_words for word in words)\n",
    "\n",
    "def count_first_person_pronouns(story, pronouns):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in pronouns for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86693a57-ccb7-402b-b006-911ec6764d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "def init_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, model, tokenizer, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "# Function to add probabilities to DataFrame\n",
    "def add_probabilities_to_df(df, index, probabilities):\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "    return df\n",
    "\n",
    "# Function to calculate Sequentiality scores\n",
    "def calculate_sequentiality(df, history_sizes):\n",
    "    for h in history_sizes:\n",
    "        if h == 0: continue\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df[seq_key] = df[f'probability_history_size{h}'] - df['probability_history_size0']\n",
    "    return df\n",
    "\n",
    "# Function to extract linguistic features\n",
    "def extract_linguistic_features(df):\n",
    "    df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['avg_word_length'] = df['story'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "    df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "    df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "    df['sensory_word_count'] = df['story'].apply(lambda x: sum(word in {'see', 'hear', 'touch', 'taste', 'smell', 'sight', 'sound', 'texture', 'aroma', 'flavor'} for word in x.split()))\n",
    "    df['first_person_pronoun_count'] = df['story'].apply(lambda x: sum(word.lower() in {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'} for word in x.split()))\n",
    "    df['past_tense_verb_count'] = df['story'].apply(lambda x: sum(tag.startswith('VBD') for word, tag in nltk.pos_tag(nltk.word_tokenize(x))))\n",
    "    return df\n",
    "\n",
    "# Function to train the classification model and validate it\n",
    "def train_and_validate(X_train, X_val, y_train, y_val):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train.fillna(0), y_train)\n",
    "    y_pred = clf.predict(X_val.fillna(0))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def run_pipeline():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"hippoCorpusV2.csv\").head(3500)\n",
    "\n",
    "    # Initialize model\n",
    "    model, tokenizer = init_model()\n",
    "\n",
    "    # Process each story and calculate probabilities and Sequentiality scores\n",
    "    history_sizes = [0, 1, 2, 3, 4, 5]\n",
    "    for index, row in df.iterrows():\n",
    "        probabilities = calculate_probabilities(row['story'], model, tokenizer, history_sizes)\n",
    "        df = add_probabilities_to_df(df, index, probabilities)\n",
    "    df = calculate_sequentiality(df, history_sizes)\n",
    "\n",
    "    # Extract linguistic features\n",
    "    df = extract_linguistic_features(df)\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    feature_cols = ['word_count', 'sentence_count', 'avg_word_length', 'lexical_diversity', 'punctuation_count'] + \\\n",
    "                   [f'probability_history_size{i}' for i in history_sizes] + \\\n",
    "                   [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and validate the classification model\n",
    "    accuracy = train_and_validate(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7efc4-a42c-4728-83a4-8e5ddfac69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56edc49-d12e-463d-88d2-20f938740892",
   "metadata": {},
   "source": [
    "## Classifier model with 10 Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb5231-2900-4b06-968a-82601888c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(story):\n",
    "    return len(story.split())\n",
    "\n",
    "def count_sentences(story):\n",
    "    return len(sent_tokenize(story))\n",
    "\n",
    "def average_word_length(story):\n",
    "    words = story.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def lexical_diversity(story):\n",
    "    words = story.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def punctuation_count(story):\n",
    "    return sum(1 for char in story if char in string.punctuation)\n",
    "\n",
    "def average_sentence_length(story):\n",
    "    words = story.split()\n",
    "    sentences = sent_tokenize(story)\n",
    "    return len(words) / len(sentences) if sentences else 0\n",
    "\n",
    "def count_sensory_words(story, sensory_words):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in sensory_words for word in words)\n",
    "\n",
    "def count_first_person_pronouns(story, pronouns):\n",
    "    words = word_tokenize(story)\n",
    "    return sum(word.lower() in pronouns for word in words)\n",
    "\n",
    "def count_emotion_sentences(stories):\n",
    "    return sum(TextBlob(sentence).sentiment.polarity != 0 for sentence in sent_tokenize(stories))\n",
    "\n",
    "def count_dialogue_tags(story, tags):\n",
    "    words = word_tokenize(story)\n",
    "    tagged_words = pos_tag(words)\n",
    "    return sum(tag.startswith('VB') and word.lower() in tags for word, tag in tagged_words)\n",
    "\n",
    "def count_past_tense_verbs(story):\n",
    "    words = word_tokenize(story)\n",
    "    tagged_words = pos_tag(words)\n",
    "    return sum(tag in ['VBD', 'VBN'] for word, tag in tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1885c-ccb1-4fcc-a8fc-751bf932fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "def init_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, model, tokenizer, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "# Function to add probabilities to DataFrame\n",
    "def add_probabilities_to_df(df, index, probabilities):\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "    return df\n",
    "\n",
    "# Function to calculate Sequentiality scores\n",
    "def calculate_sequentiality(df, history_sizes):\n",
    "    for h in history_sizes:\n",
    "        if h == 0: continue\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df[seq_key] = df[f'probability_history_size{h}'] - df['probability_history_size0']\n",
    "    return df\n",
    "\n",
    "# Function to extract linguistic features\n",
    "def extract_linguistic_features(df):\n",
    "    df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "    df['sensory_word_count'] = df['story'].apply(lambda x: sum(word in {'see', 'hear', 'touch', 'taste', 'smell', 'sight', 'sound', 'texture', 'aroma', 'flavor'} for word in x.split()))\n",
    "    df['first_person_pronoun_count'] = df['story'].apply(lambda x: sum(word.lower() in {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'} for word in x.split()))\n",
    "    df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['past_tense_verb_count'] = df['story'].apply(lambda x: sum(tag.startswith('VBD') for word, tag in nltk.pos_tag(nltk.word_tokenize(x))))\n",
    "    df['emotion_word_count'] = df['story'].apply(lambda x: sum(word.lower() in {'happy', 'sad', 'angry', 'joyful', 'depressed', 'excited', 'fearful', 'anxious', 'content', 'disappointed'} for word in x.split()))\n",
    "    df['dialogue_tag_count'] = df['story'].apply(lambda x: sum(word.lower() in {'said', 'asked', 'replied', 'shouted', 'whispered', 'murmured', 'screamed', 'yelled', 'muttered', 'uttered', 'exclaimed'} for word in x.split()))\n",
    "    df['avg_word_length'] = df['story'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "    df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to train the classification model and validate it\n",
    "def train_and_validate(X_train, X_val, y_train, y_val):\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train.fillna(0), y_train)\n",
    "    y_pred = clf.predict(X_val.fillna(0))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def run_pipeline():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"hippoCorpusV2.csv\").head(3500)\n",
    "\n",
    "    # Initialize model\n",
    "    model, tokenizer = init_model()\n",
    "\n",
    "    # Process each story and calculate probabilities and Sequentiality scores\n",
    "    history_sizes = [0, 1, 2, 3, 4, 5]\n",
    "    for index, row in df.iterrows():\n",
    "        probabilities = calculate_probabilities(row['story'], model, tokenizer, history_sizes)\n",
    "        df = add_probabilities_to_df(df, index, probabilities)\n",
    "    df = calculate_sequentiality(df, history_sizes)\n",
    "\n",
    "    # Extract linguistic features\n",
    "    df = extract_linguistic_features(df)\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    feature_cols = ['word_count', 'sentence_count', 'avg_word_length', 'lexical_diversity', 'punctuation_count','past_tense_verb_count','emotion_word_count','first_person_pronoun_count',] + \\\n",
    "                   [f'probability_history_size{i}' for i in history_sizes] + \\\n",
    "                   [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and validate the classification model\n",
    "    accuracy = train_and_validate(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f5215-9d0b-47bc-be88-dbe363a8e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def preprocess_data(dataframe, feature_cols, target, class_map):\n",
    "    features = dataframe[feature_cols + [target]]\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    features[feature_cols] = imputer.fit_transform(features[feature_cols])\n",
    "    features[target] = features[target].map(class_map)\n",
    "    return features\n",
    "\n",
    "def split_data(features, target, test_size, stratify, random_state):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=test_size, stratify=stratify, random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=random_state)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def standardize_and_poly_transform(X_train, X_val, degree):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_val_poly = poly.transform(X_val_scaled)\n",
    "    return X_train_poly, X_val_poly\n",
    "\n",
    "def perform_grid_search(X_train, y_train, param_grid, cv_splits, scoring_method):\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=StratifiedKFold(cv_splits), scoring=scoring_method)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "# Define constants and parameters\n",
    "DATA_FILEPATH = \"hippoCorpusV2.csv\"\n",
    "FEATURE_COLUMNS = [\"probability_history_size0\", \"probability_history_size2\",\"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\"dialogue_tag_count\",\"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\"]\n",
    "TARGET_COLUMN = 'memType'\n",
    "CLASS_MAPPING = {'imagined': 0, 'recalled': 1}\n",
    "TEST_SIZE = 0.4\n",
    "RANDOM_STATE = 42\n",
    "POLY_DEGREE = 2\n",
    "PARAM_GRID = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "CV_SPLITS = 5\n",
    "SCORING_METHOD = 'accuracy'\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Data loading and preprocessing\n",
    "    story_data = load_data(DATA_FILEPATH)\n",
    "    processed_data = preprocess_data(story_data, FEATURE_COLUMNS, TARGET_COLUMN, CLASS_MAPPING)\n",
    "    \n",
    "    # Data splitting\n",
    "    X_train, X_val, _, y_train, y_val, _ = split_data(processed_data[FEATURE_COLUMNS], processed_data[TARGET_COLUMN], TEST_SIZE, processed_data[TARGET_COLUMN], RANDOM_STATE)\n",
    "    \n",
    "    # Feature scaling and transformation\n",
    "    X_train_transformed, X_val_transformed = standardize_and_poly_transform(X_train, X_val, POLY_DEGREE)\n",
    "    \n",
    "    # Model training and hyperparameter tuning\n",
    "    best_model = perform_grid_search(X_train_transformed, y_train, PARAM_GRID, CV_SPLITS, SCORING_METHOD)\n",
    "    \n",
    "    # Model evaluation\n",
    "    validation_accuracy = evaluate_model(best_model, X_val_transformed, y_val)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b110773-510d-4d49-9721-4fb435f173c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
